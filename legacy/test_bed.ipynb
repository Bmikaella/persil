{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from persil.data_operators import ModelPerformanceSaver\n",
    "import pandas as pd\n",
    "import json\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['learning_rate', 'batch_size', 'kernels_count', 'sentences_count', 'hidden_layer1', 'alpha', 'regularization_type', 'decay_rate', 'decay_epoch']\n['learning_rate', 'batch_size', 'kernels_count', 'sentences_count', 'hidden_layer1', 'alpha', 'regularization_type', 'decay_rate', 'decay_epoch']\n/home/mibo/save.csv\n/home/mibo/results_FINAL_all_traits.csv\n"
    }
   ],
   "source": [
    "MODELS_PARAMETER_COLUMNS = ['learning_rate', 'batch_size',\\\n",
    "                           'kernels_count', 'sentences_count', 'hidden_layer1', 'alpha', 'regularization_type',\\\n",
    "                            'decay_rate', 'decay_epoch']\n",
    "b = ModelPerformanceSaver(columns=MODELS_PARAMETER_COLUMNS, id_columns=MODELS_PARAMETER_COLUMNS, save_location='/home/mibo/save.csv', import_location='/home/mibo/results_FINAL_all_traits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'dcb2bdd64c44315545623781aae14936'"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.create_new_entry('testing smth', 'model1', 'introverted', 5, 'run4', {'learning_rate' : 0.014, 'batch_size' : 64,\\\n",
    "                           'kernels_count' : 5, 'sentences_count' : 5, 'hidden_layer1' : 5, 'alpha' : 0.002, 'regularization_type': 'L1',\\\n",
    "                            'decay_rate' : 89, 'decay_epoch' : 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>batch_size</th>\n      <th>decay_epoch</th>\n      <th>decay_rate</th>\n      <th>epoch</th>\n      <th>fold</th>\n      <th>hash_id</th>\n      <th>kernels_count</th>\n      <th>learning_rate</th>\n      <th>mbti_trait</th>\n      <th>...</th>\n      <th>val_precision_0</th>\n      <th>val_precision_1</th>\n      <th>val_precision_macro</th>\n      <th>val_recall_0</th>\n      <th>val_recall_1</th>\n      <th>val_recall_macro</th>\n      <th>alpha</th>\n      <th>experiments_name</th>\n      <th>hidden_layer1</th>\n      <th>regularization_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33.0</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>74.0</td>\n      <td>0</td>\n      <td>62db84edd1d577ff9600988957698c14</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.504854</td>\n      <td>0.760479</td>\n      <td>0.632667</td>\n      <td>0.464286</td>\n      <td>0.760479</td>\n      <td>0.626553</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39.0</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>85.0</td>\n      <td>1</td>\n      <td>22177ae8a52d1c52a6e4f6605ae89652</td>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.514680</td>\n      <td>0.744578</td>\n      <td>0.629629</td>\n      <td>0.584314</td>\n      <td>0.744578</td>\n      <td>0.635872</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41.0</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>70.0</td>\n      <td>2</td>\n      <td>54ccf65429417b0fc8fe71c8977f7088</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.585561</td>\n      <td>0.735099</td>\n      <td>0.660330</td>\n      <td>0.438878</td>\n      <td>0.735099</td>\n      <td>0.636284</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45.0</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>75.0</td>\n      <td>3</td>\n      <td>a7fab795c93e07114c2529c57f91f190</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.564732</td>\n      <td>0.777549</td>\n      <td>0.671141</td>\n      <td>0.539446</td>\n      <td>0.777549</td>\n      <td>0.667091</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49.0</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>79.0</td>\n      <td>4</td>\n      <td>ed19e8c2165441d82a0fc67257beed4e</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.557269</td>\n      <td>0.738860</td>\n      <td>0.648064</td>\n      <td>0.500990</td>\n      <td>0.738860</td>\n      <td>0.640539</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14.0</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>66.0</td>\n      <td>0</td>\n      <td>a3ecaaa60199d0f8e5ea70ba7a2f500e</td>\n      <td>12</td>\n      <td>0.000005</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.520891</td>\n      <td>0.637915</td>\n      <td>0.579403</td>\n      <td>0.328647</td>\n      <td>0.637915</td>\n      <td>0.562548</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>17.0</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>eb52aa07e6e3a1143ffcaeb47f2d306e</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.517413</td>\n      <td>0.620033</td>\n      <td>0.568723</td>\n      <td>0.184725</td>\n      <td>0.620033</td>\n      <td>0.535034</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>21.0</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>116.0</td>\n      <td>2</td>\n      <td>99fdca1126bb950ef6f509def11c50ad</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.486842</td>\n      <td>0.636537</td>\n      <td>0.561689</td>\n      <td>0.326279</td>\n      <td>0.636537</td>\n      <td>0.550292</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>26.0</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>96.0</td>\n      <td>3</td>\n      <td>cf1f844f9db7745841419b6474fb34df</td>\n      <td>12</td>\n      <td>0.000005</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.536458</td>\n      <td>0.620290</td>\n      <td>0.578374</td>\n      <td>0.343907</td>\n      <td>0.620290</td>\n      <td>0.563417</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>31.0</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>14.0</td>\n      <td>4</td>\n      <td>1f6484341563f0449157c8ccc3ba705b</td>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.430986</td>\n      <td>0.623413</td>\n      <td>0.527200</td>\n      <td>0.534031</td>\n      <td>0.623413</td>\n      <td>0.528245</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10.0</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>44.0</td>\n      <td>0</td>\n      <td>525a5e8ed14a64396959b985c862a12a</td>\n      <td>8</td>\n      <td>0.000001</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.282353</td>\n      <td>0.810056</td>\n      <td>0.546204</td>\n      <td>0.320000</td>\n      <td>0.810056</td>\n      <td>0.550485</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12.0</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>25.0</td>\n      <td>1</td>\n      <td>cb3f80e3f6f052a37fbf3eff41ea4605</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.255973</td>\n      <td>0.786738</td>\n      <td>0.521356</td>\n      <td>0.239617</td>\n      <td>0.786738</td>\n      <td>0.520356</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18.0</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>be7373fc4ebe9f7b612fc3fd847ee669</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.222222</td>\n      <td>0.806854</td>\n      <td>0.514538</td>\n      <td>0.358621</td>\n      <td>0.806854</td>\n      <td>0.519801</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>22.0</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>25.0</td>\n      <td>3</td>\n      <td>8d46dff8f4702f3e83be7c0bf2cffcd3</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.305970</td>\n      <td>0.798444</td>\n      <td>0.552207</td>\n      <td>0.136667</td>\n      <td>0.798444</td>\n      <td>0.526778</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>27.0</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>20.0</td>\n      <td>4</td>\n      <td>75113e2ea1767604c457913178134f06</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.256798</td>\n      <td>0.810662</td>\n      <td>0.533730</td>\n      <td>0.292096</td>\n      <td>0.810662</td>\n      <td>0.537006</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3.0</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>bff1d7ef58ce9a71a0b9699e2a70994d</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.130000</td>\n      <td>0.892092</td>\n      <td>0.511046</td>\n      <td>0.165605</td>\n      <td>0.892092</td>\n      <td>0.513590</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7.0</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>42.0</td>\n      <td>1</td>\n      <td>8a501006d109da584e368d671cff6e97</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.181034</td>\n      <td>0.885538</td>\n      <td>0.533286</td>\n      <td>0.124260</td>\n      <td>0.885538</td>\n      <td>0.523824</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>10.0</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>48.0</td>\n      <td>2</td>\n      <td>8c3913539b7edac19bc17235f3c65c66</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.181102</td>\n      <td>0.890337</td>\n      <td>0.535720</td>\n      <td>0.138554</td>\n      <td>0.890337</td>\n      <td>0.528170</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>14.0</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>0084f2350618229dcdcbbc39c8ea9925</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.243243</td>\n      <td>0.892185</td>\n      <td>0.567714</td>\n      <td>0.056962</td>\n      <td>0.892185</td>\n      <td>0.517379</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19.0</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>44.0</td>\n      <td>4</td>\n      <td>9f5457203f37245364f4e91c845a4fe0</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.137931</td>\n      <td>0.893976</td>\n      <td>0.515953</td>\n      <td>0.153846</td>\n      <td>0.893976</td>\n      <td>0.517541</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>NaN</td>\n      <td>64</td>\n      <td>0.5</td>\n      <td>89.00</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>dcb2bdd64c44315545623781aae14936</td>\n      <td>5</td>\n      <td>0.014000</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.002</td>\n      <td>testing smth</td>\n      <td>5.0</td>\n      <td>L1</td>\n    </tr>\n  </tbody>\n</table>\n<p>21 rows × 31 columns</p>\n</div>",
      "text/plain": "    Unnamed: 0  batch_size  decay_epoch  decay_rate  epoch  fold  \\\n0         33.0          64         70.0        0.80   74.0     0   \n1         39.0          64         70.0        0.80   85.0     1   \n2         41.0          64         70.0        0.80   70.0     2   \n3         45.0          64         70.0        0.80   75.0     3   \n4         49.0          64         70.0        0.80   79.0     4   \n5         14.0          64         10.0        0.90   66.0     0   \n6         17.0          64         10.0        0.90   47.0     1   \n7         21.0          64         10.0        0.90  116.0     2   \n8         26.0          64         10.0        0.90   96.0     3   \n9         31.0          64         10.0        0.90   14.0     4   \n10        10.0          64         30.0        0.90   44.0     0   \n11        12.0          64         30.0        0.90   25.0     1   \n12        18.0          64         30.0        0.90    0.0     2   \n13        22.0          64         30.0        0.90   25.0     3   \n14        27.0          64         30.0        0.90   20.0     4   \n15         3.0         128         40.0        0.95    0.0     0   \n16         7.0         128         40.0        0.95   42.0     1   \n17        10.0         128         40.0        0.95   48.0     2   \n18        14.0         128         40.0        0.95    2.0     3   \n19        19.0         128         40.0        0.95   44.0     4   \n20         NaN          64          0.5       89.00    NaN     5   \n\n                             hash_id  kernels_count  learning_rate  \\\n0   62db84edd1d577ff9600988957698c14             12       0.000010   \n1   22177ae8a52d1c52a6e4f6605ae89652             12       0.000001   \n2   54ccf65429417b0fc8fe71c8977f7088             12       0.000010   \n3   a7fab795c93e07114c2529c57f91f190             12       0.000010   \n4   ed19e8c2165441d82a0fc67257beed4e             12       0.000010   \n5   a3ecaaa60199d0f8e5ea70ba7a2f500e             12       0.000005   \n6   eb52aa07e6e3a1143ffcaeb47f2d306e             12       0.000010   \n7   99fdca1126bb950ef6f509def11c50ad             12       0.000010   \n8   cf1f844f9db7745841419b6474fb34df             12       0.000005   \n9   1f6484341563f0449157c8ccc3ba705b             12       0.000001   \n10  525a5e8ed14a64396959b985c862a12a              8       0.000001   \n11  cb3f80e3f6f052a37fbf3eff41ea4605              8       0.000100   \n12  be7373fc4ebe9f7b612fc3fd847ee669              8       0.000010   \n13  8d46dff8f4702f3e83be7c0bf2cffcd3              8       0.000100   \n14  75113e2ea1767604c457913178134f06              8       0.000100   \n15  bff1d7ef58ce9a71a0b9699e2a70994d              8       0.000010   \n16  8a501006d109da584e368d671cff6e97              8       0.000010   \n17  8c3913539b7edac19bc17235f3c65c66              8       0.000100   \n18  0084f2350618229dcdcbbc39c8ea9925              8       0.000100   \n19  9f5457203f37245364f4e91c845a4fe0              8       0.000010   \n20  dcb2bdd64c44315545623781aae14936              5       0.014000   \n\n     mbti_trait  ... val_precision_0 val_precision_1  val_precision_macro  \\\n0      thinking  ...        0.504854        0.760479             0.632667   \n1      thinking  ...        0.514680        0.744578             0.629629   \n2      thinking  ...        0.585561        0.735099             0.660330   \n3      thinking  ...        0.564732        0.777549             0.671141   \n4      thinking  ...        0.557269        0.738860             0.648064   \n5    perceiving  ...        0.520891        0.637915             0.579403   \n6    perceiving  ...        0.517413        0.620033             0.568723   \n7    perceiving  ...        0.486842        0.636537             0.561689   \n8    perceiving  ...        0.536458        0.620290             0.578374   \n9    perceiving  ...        0.430986        0.623413             0.527200   \n10  introverted  ...        0.282353        0.810056             0.546204   \n11  introverted  ...        0.255973        0.786738             0.521356   \n12  introverted  ...        0.222222        0.806854             0.514538   \n13  introverted  ...        0.305970        0.798444             0.552207   \n14  introverted  ...        0.256798        0.810662             0.533730   \n15    intuitive  ...        0.130000        0.892092             0.511046   \n16    intuitive  ...        0.181034        0.885538             0.533286   \n17    intuitive  ...        0.181102        0.890337             0.535720   \n18    intuitive  ...        0.243243        0.892185             0.567714   \n19    intuitive  ...        0.137931        0.893976             0.515953   \n20  introverted  ...             NaN             NaN                  NaN   \n\n    val_recall_0  val_recall_1  val_recall_macro  alpha  experiments_name  \\\n0       0.464286      0.760479          0.626553    NaN               NaN   \n1       0.584314      0.744578          0.635872    NaN               NaN   \n2       0.438878      0.735099          0.636284    NaN               NaN   \n3       0.539446      0.777549          0.667091    NaN               NaN   \n4       0.500990      0.738860          0.640539    NaN               NaN   \n5       0.328647      0.637915          0.562548    NaN               NaN   \n6       0.184725      0.620033          0.535034    NaN               NaN   \n7       0.326279      0.636537          0.550292    NaN               NaN   \n8       0.343907      0.620290          0.563417    NaN               NaN   \n9       0.534031      0.623413          0.528245    NaN               NaN   \n10      0.320000      0.810056          0.550485    NaN               NaN   \n11      0.239617      0.786738          0.520356    NaN               NaN   \n12      0.358621      0.806854          0.519801    NaN               NaN   \n13      0.136667      0.798444          0.526778    NaN               NaN   \n14      0.292096      0.810662          0.537006    NaN               NaN   \n15      0.165605      0.892092          0.513590    NaN               NaN   \n16      0.124260      0.885538          0.523824    NaN               NaN   \n17      0.138554      0.890337          0.528170    NaN               NaN   \n18      0.056962      0.892185          0.517379    NaN               NaN   \n19      0.153846      0.893976          0.517541    NaN               NaN   \n20           NaN           NaN               NaN  0.002      testing smth   \n\n    hidden_layer1  regularization_type  \n0             NaN                  NaN  \n1             NaN                  NaN  \n2             NaN                  NaN  \n3             NaN                  NaN  \n4             NaN                  NaN  \n5             NaN                  NaN  \n6             NaN                  NaN  \n7             NaN                  NaN  \n8             NaN                  NaN  \n9             NaN                  NaN  \n10            NaN                  NaN  \n11            NaN                  NaN  \n12            NaN                  NaN  \n13            NaN                  NaN  \n14            NaN                  NaN  \n15            NaN                  NaN  \n16            NaN                  NaN  \n17            NaN                  NaN  \n18            NaN                  NaN  \n19            NaN                  NaN  \n20            5.0                   L1  \n\n[21 rows x 31 columns]"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.flush_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'learning_rates': [0.02], 'batch_sizes': [7, 9, 1], 'kernels_counts': [8, 9], 'sentences_counts': [2, 8], 'hidden_layers_1': [2, 5], 'alphas': [0.02, 0.027], 'regularization_types': ['L1']}\n[('learning_rates', 0.02)]\n[('batch_sizes', 7), ('batch_sizes', 9), ('batch_sizes', 1)]\n[('kernels_counts', 8), ('kernels_counts', 9)]\n[('sentences_counts', 2), ('sentences_counts', 8)]\n[('hidden_layers_1', 2), ('hidden_layers_1', 5)]\n[('alphas', 0.02), ('alphas', 0.027)]\n[('regularization_types', 'L1')]\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 7, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 9, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 8, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 2, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 2, 'alphas': 0.027, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.02, 'regularization_types': 'L1'}\n{'learning_rates': 0.02, 'batch_sizes': 1, 'kernels_counts': 9, 'sentences_counts': 8, 'hidden_layers_1': 5, 'alphas': 0.027, 'regularization_types': 'L1'}\n"
    }
   ],
   "source": [
    "models_data = json.load(open('/home/mibo/Projects/PERO/persil/modelsData', 'r'))\n",
    "models_parameters = models_data['models_parameters']\n",
    "print(models_parameters)\n",
    "a = [list(zip([label] * len(values), values)) for label, values in models_parameters.items()] \n",
    "c = product(*a)\n",
    "for k in a:\n",
    "    print(k)\n",
    "for t in c:\n",
    "    print(dict(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Independent\n_nephilim_\nicebrotha\n"
    },
    {
     "data": {
      "text/plain": "{}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kaka (sentences, t, y): \n",
    "    print(sentences['author'].values[0])\n",
    "\n",
    "data_df = pd.read_csv('/home/mibo/Datasets/embedded_comments_last_100_min_20com_per_auth_w_wc_10_200_no_mbti.csv', nrows=10)\n",
    "input_df = dict(data_df.sort_values(by=['author', 'Unnamed: 0']).groupby(['author'])\\\n",
    "            .apply(lambda sentences : kaka(sentences, 25, 45)))\n",
    "input_df\n",
    "        "
   ]
  }
 ]
}