{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "parser_f() got an unexpected keyword argument 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-640a5c633a2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                            \u001b[0;34m'kernels_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentences_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hidden_layer1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alpha'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'regularization_type'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             'decay_rate', 'decay_epoch']\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelDataSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELS_PARAMETER_COLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODELS_PARAMETER_COLUMNS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/mibo/save.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/home/mibo/results_FINAL_all_traits.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/PERO/persil/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, columns, id_columns, save_location, import_location)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_location\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: parser_f() got an unexpected keyword argument 'index'"
     ]
    }
   ],
   "source": [
    "\n",
    "MODELS_PARAMETER_COLUMNS = ['learning_rate', 'batch_size',\\\n",
    "                           'kernels_count', 'sentences_count', 'hidden_layer1', 'alpha', 'regularization_type',\\\n",
    "                            'decay_rate', 'decay_epoch']\n",
    "a = ModelDataSaver(columns=MODELS_PARAMETER_COLUMNS, id_columns=MODELS_PARAMETER_COLUMNS, save_location='/home/mibo/save.csv', import_location='/home/mibo/results_FINAL_all_traits.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'3f633df0a08aa2320565239862292d05'"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.create_new_entry('model1', 'introverted', 5, 'run4', {'learning_rate' : 0.014, 'batch_size' : 64,\\\n",
    "                           'kernels_count' : 5, 'sentences_count' : 5, 'hidden_layer1' : 5, 'alpha' : 0.002, 'regularization_type': 'L1',\\\n",
    "                            'decay_rate' : 89, 'decay_epoch' : 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>batch_size</th>\n      <th>decay_epoch</th>\n      <th>decay_rate</th>\n      <th>epoch</th>\n      <th>fold</th>\n      <th>hash_id</th>\n      <th>kernels_count</th>\n      <th>learning_rate</th>\n      <th>mbti_trait</th>\n      <th>...</th>\n      <th>test_recall_0</th>\n      <th>test_recall_1</th>\n      <th>test_recall_macro</th>\n      <th>val_f1</th>\n      <th>val_precision_0</th>\n      <th>val_precision_1</th>\n      <th>val_precision_macro</th>\n      <th>val_recall_0</th>\n      <th>val_recall_1</th>\n      <th>val_recall_macro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>33</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>74.0</td>\n      <td>0</td>\n      <td>62db84edd1d577ff9600988957698c14</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.472012</td>\n      <td>0.793651</td>\n      <td>0.632831</td>\n      <td>0.629056</td>\n      <td>0.504854</td>\n      <td>0.760479</td>\n      <td>0.632667</td>\n      <td>0.464286</td>\n      <td>0.760479</td>\n      <td>0.626553</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>85.0</td>\n      <td>1</td>\n      <td>22177ae8a52d1c52a6e4f6605ae89652</td>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.560510</td>\n      <td>0.659950</td>\n      <td>0.610230</td>\n      <td>0.631078</td>\n      <td>0.514680</td>\n      <td>0.744578</td>\n      <td>0.629629</td>\n      <td>0.584314</td>\n      <td>0.744578</td>\n      <td>0.635872</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>41</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>70.0</td>\n      <td>2</td>\n      <td>54ccf65429417b0fc8fe71c8977f7088</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.459596</td>\n      <td>0.826750</td>\n      <td>0.643173</td>\n      <td>0.641508</td>\n      <td>0.585561</td>\n      <td>0.735099</td>\n      <td>0.660330</td>\n      <td>0.438878</td>\n      <td>0.735099</td>\n      <td>0.636284</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>45</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>75.0</td>\n      <td>3</td>\n      <td>a7fab795c93e07114c2529c57f91f190</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.473354</td>\n      <td>0.782493</td>\n      <td>0.627924</td>\n      <td>0.668924</td>\n      <td>0.564732</td>\n      <td>0.777549</td>\n      <td>0.671141</td>\n      <td>0.539446</td>\n      <td>0.777549</td>\n      <td>0.667091</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>49</td>\n      <td>64</td>\n      <td>70.0</td>\n      <td>0.80</td>\n      <td>79.0</td>\n      <td>4</td>\n      <td>ed19e8c2165441d82a0fc67257beed4e</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>thinking</td>\n      <td>...</td>\n      <td>0.491086</td>\n      <td>0.807459</td>\n      <td>0.649272</td>\n      <td>0.643274</td>\n      <td>0.557269</td>\n      <td>0.738860</td>\n      <td>0.648064</td>\n      <td>0.500990</td>\n      <td>0.738860</td>\n      <td>0.640539</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>14</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>66.0</td>\n      <td>0</td>\n      <td>a3ecaaa60199d0f8e5ea70ba7a2f500e</td>\n      <td>12</td>\n      <td>0.000005</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.299745</td>\n      <td>0.789318</td>\n      <td>0.544531</td>\n      <td>0.555719</td>\n      <td>0.520891</td>\n      <td>0.637915</td>\n      <td>0.579403</td>\n      <td>0.328647</td>\n      <td>0.637915</td>\n      <td>0.562548</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>17</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>47.0</td>\n      <td>1</td>\n      <td>eb52aa07e6e3a1143ffcaeb47f2d306e</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.176471</td>\n      <td>0.885068</td>\n      <td>0.530769</td>\n      <td>0.500780</td>\n      <td>0.517413</td>\n      <td>0.620033</td>\n      <td>0.568723</td>\n      <td>0.184725</td>\n      <td>0.620033</td>\n      <td>0.535034</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>21</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>116.0</td>\n      <td>2</td>\n      <td>99fdca1126bb950ef6f509def11c50ad</td>\n      <td>12</td>\n      <td>0.000010</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.348315</td>\n      <td>0.760040</td>\n      <td>0.554177</td>\n      <td>0.544701</td>\n      <td>0.486842</td>\n      <td>0.636537</td>\n      <td>0.561689</td>\n      <td>0.326279</td>\n      <td>0.636537</td>\n      <td>0.550292</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>26</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>96.0</td>\n      <td>3</td>\n      <td>cf1f844f9db7745841419b6474fb34df</td>\n      <td>12</td>\n      <td>0.000005</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.297931</td>\n      <td>0.767241</td>\n      <td>0.532586</td>\n      <td>0.555654</td>\n      <td>0.536458</td>\n      <td>0.620290</td>\n      <td>0.578374</td>\n      <td>0.343907</td>\n      <td>0.620290</td>\n      <td>0.563417</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>31</td>\n      <td>64</td>\n      <td>10.0</td>\n      <td>0.90</td>\n      <td>14.0</td>\n      <td>4</td>\n      <td>1f6484341563f0449157c8ccc3ba705b</td>\n      <td>12</td>\n      <td>0.000001</td>\n      <td>perceiving</td>\n      <td>...</td>\n      <td>0.522634</td>\n      <td>0.497598</td>\n      <td>0.510116</td>\n      <td>0.522748</td>\n      <td>0.430986</td>\n      <td>0.623413</td>\n      <td>0.527200</td>\n      <td>0.534031</td>\n      <td>0.623413</td>\n      <td>0.528245</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>44.0</td>\n      <td>0</td>\n      <td>525a5e8ed14a64396959b985c862a12a</td>\n      <td>8</td>\n      <td>0.000001</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.269122</td>\n      <td>0.766297</td>\n      <td>0.517709</td>\n      <td>0.547623</td>\n      <td>0.282353</td>\n      <td>0.810056</td>\n      <td>0.546204</td>\n      <td>0.320000</td>\n      <td>0.810056</td>\n      <td>0.550485</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>12</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>25.0</td>\n      <td>1</td>\n      <td>cb3f80e3f6f052a37fbf3eff41ea4605</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.334152</td>\n      <td>0.719547</td>\n      <td>0.526850</td>\n      <td>0.520688</td>\n      <td>0.255973</td>\n      <td>0.786738</td>\n      <td>0.521356</td>\n      <td>0.239617</td>\n      <td>0.786738</td>\n      <td>0.520356</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>18</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>0.0</td>\n      <td>2</td>\n      <td>be7373fc4ebe9f7b612fc3fd847ee669</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.361930</td>\n      <td>0.689888</td>\n      <td>0.525909</td>\n      <td>0.506500</td>\n      <td>0.222222</td>\n      <td>0.806854</td>\n      <td>0.514538</td>\n      <td>0.358621</td>\n      <td>0.806854</td>\n      <td>0.519801</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>22</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>25.0</td>\n      <td>3</td>\n      <td>8d46dff8f4702f3e83be7c0bf2cffcd3</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.139037</td>\n      <td>0.883871</td>\n      <td>0.511454</td>\n      <td>0.521259</td>\n      <td>0.305970</td>\n      <td>0.798444</td>\n      <td>0.552207</td>\n      <td>0.136667</td>\n      <td>0.798444</td>\n      <td>0.526778</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>27</td>\n      <td>64</td>\n      <td>30.0</td>\n      <td>0.90</td>\n      <td>20.0</td>\n      <td>4</td>\n      <td>75113e2ea1767604c457913178134f06</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>introverted</td>\n      <td>...</td>\n      <td>0.430137</td>\n      <td>0.614235</td>\n      <td>0.522186</td>\n      <td>0.534670</td>\n      <td>0.256798</td>\n      <td>0.810662</td>\n      <td>0.533730</td>\n      <td>0.292096</td>\n      <td>0.810662</td>\n      <td>0.537006</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>3</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>bff1d7ef58ce9a71a0b9699e2a70994d</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.127358</td>\n      <td>0.852811</td>\n      <td>0.490085</td>\n      <td>0.511113</td>\n      <td>0.130000</td>\n      <td>0.892092</td>\n      <td>0.511046</td>\n      <td>0.165605</td>\n      <td>0.892092</td>\n      <td>0.513590</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>7</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>42.0</td>\n      <td>1</td>\n      <td>8a501006d109da584e368d671cff6e97</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.082927</td>\n      <td>0.924411</td>\n      <td>0.503669</td>\n      <td>0.525717</td>\n      <td>0.181034</td>\n      <td>0.885538</td>\n      <td>0.533286</td>\n      <td>0.124260</td>\n      <td>0.885538</td>\n      <td>0.523824</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>10</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>48.0</td>\n      <td>2</td>\n      <td>8c3913539b7edac19bc17235f3c65c66</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.067010</td>\n      <td>0.914795</td>\n      <td>0.490903</td>\n      <td>0.530425</td>\n      <td>0.181102</td>\n      <td>0.890337</td>\n      <td>0.535720</td>\n      <td>0.138554</td>\n      <td>0.890337</td>\n      <td>0.528170</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>14</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>2.0</td>\n      <td>3</td>\n      <td>0084f2350618229dcdcbbc39c8ea9925</td>\n      <td>8</td>\n      <td>0.000100</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.017751</td>\n      <td>0.983125</td>\n      <td>0.500438</td>\n      <td>0.512669</td>\n      <td>0.243243</td>\n      <td>0.892185</td>\n      <td>0.567714</td>\n      <td>0.056962</td>\n      <td>0.892185</td>\n      <td>0.517379</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>128</td>\n      <td>40.0</td>\n      <td>0.95</td>\n      <td>44.0</td>\n      <td>4</td>\n      <td>9f5457203f37245364f4e91c845a4fe0</td>\n      <td>8</td>\n      <td>0.000010</td>\n      <td>intuitive</td>\n      <td>...</td>\n      <td>0.161572</td>\n      <td>0.862427</td>\n      <td>0.512000</td>\n      <td>0.516507</td>\n      <td>0.137931</td>\n      <td>0.893976</td>\n      <td>0.515953</td>\n      <td>0.153846</td>\n      <td>0.893976</td>\n      <td>0.517541</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows Ã— 27 columns</p>\n</div>",
      "text/plain": "    Unnamed: 0  batch_size  decay_epoch  decay_rate  epoch  fold  \\\n0           33          64         70.0        0.80   74.0     0   \n1           39          64         70.0        0.80   85.0     1   \n2           41          64         70.0        0.80   70.0     2   \n3           45          64         70.0        0.80   75.0     3   \n4           49          64         70.0        0.80   79.0     4   \n5           14          64         10.0        0.90   66.0     0   \n6           17          64         10.0        0.90   47.0     1   \n7           21          64         10.0        0.90  116.0     2   \n8           26          64         10.0        0.90   96.0     3   \n9           31          64         10.0        0.90   14.0     4   \n10          10          64         30.0        0.90   44.0     0   \n11          12          64         30.0        0.90   25.0     1   \n12          18          64         30.0        0.90    0.0     2   \n13          22          64         30.0        0.90   25.0     3   \n14          27          64         30.0        0.90   20.0     4   \n15           3         128         40.0        0.95    0.0     0   \n16           7         128         40.0        0.95   42.0     1   \n17          10         128         40.0        0.95   48.0     2   \n18          14         128         40.0        0.95    2.0     3   \n19          19         128         40.0        0.95   44.0     4   \n\n                             hash_id  kernels_count  learning_rate  \\\n0   62db84edd1d577ff9600988957698c14             12       0.000010   \n1   22177ae8a52d1c52a6e4f6605ae89652             12       0.000001   \n2   54ccf65429417b0fc8fe71c8977f7088             12       0.000010   \n3   a7fab795c93e07114c2529c57f91f190             12       0.000010   \n4   ed19e8c2165441d82a0fc67257beed4e             12       0.000010   \n5   a3ecaaa60199d0f8e5ea70ba7a2f500e             12       0.000005   \n6   eb52aa07e6e3a1143ffcaeb47f2d306e             12       0.000010   \n7   99fdca1126bb950ef6f509def11c50ad             12       0.000010   \n8   cf1f844f9db7745841419b6474fb34df             12       0.000005   \n9   1f6484341563f0449157c8ccc3ba705b             12       0.000001   \n10  525a5e8ed14a64396959b985c862a12a              8       0.000001   \n11  cb3f80e3f6f052a37fbf3eff41ea4605              8       0.000100   \n12  be7373fc4ebe9f7b612fc3fd847ee669              8       0.000010   \n13  8d46dff8f4702f3e83be7c0bf2cffcd3              8       0.000100   \n14  75113e2ea1767604c457913178134f06              8       0.000100   \n15  bff1d7ef58ce9a71a0b9699e2a70994d              8       0.000010   \n16  8a501006d109da584e368d671cff6e97              8       0.000010   \n17  8c3913539b7edac19bc17235f3c65c66              8       0.000100   \n18  0084f2350618229dcdcbbc39c8ea9925              8       0.000100   \n19  9f5457203f37245364f4e91c845a4fe0              8       0.000010   \n\n     mbti_trait  ... test_recall_0 test_recall_1  test_recall_macro    val_f1  \\\n0      thinking  ...      0.472012      0.793651           0.632831  0.629056   \n1      thinking  ...      0.560510      0.659950           0.610230  0.631078   \n2      thinking  ...      0.459596      0.826750           0.643173  0.641508   \n3      thinking  ...      0.473354      0.782493           0.627924  0.668924   \n4      thinking  ...      0.491086      0.807459           0.649272  0.643274   \n5    perceiving  ...      0.299745      0.789318           0.544531  0.555719   \n6    perceiving  ...      0.176471      0.885068           0.530769  0.500780   \n7    perceiving  ...      0.348315      0.760040           0.554177  0.544701   \n8    perceiving  ...      0.297931      0.767241           0.532586  0.555654   \n9    perceiving  ...      0.522634      0.497598           0.510116  0.522748   \n10  introverted  ...      0.269122      0.766297           0.517709  0.547623   \n11  introverted  ...      0.334152      0.719547           0.526850  0.520688   \n12  introverted  ...      0.361930      0.689888           0.525909  0.506500   \n13  introverted  ...      0.139037      0.883871           0.511454  0.521259   \n14  introverted  ...      0.430137      0.614235           0.522186  0.534670   \n15    intuitive  ...      0.127358      0.852811           0.490085  0.511113   \n16    intuitive  ...      0.082927      0.924411           0.503669  0.525717   \n17    intuitive  ...      0.067010      0.914795           0.490903  0.530425   \n18    intuitive  ...      0.017751      0.983125           0.500438  0.512669   \n19    intuitive  ...      0.161572      0.862427           0.512000  0.516507   \n\n    val_precision_0  val_precision_1  val_precision_macro  val_recall_0  \\\n0          0.504854         0.760479             0.632667      0.464286   \n1          0.514680         0.744578             0.629629      0.584314   \n2          0.585561         0.735099             0.660330      0.438878   \n3          0.564732         0.777549             0.671141      0.539446   \n4          0.557269         0.738860             0.648064      0.500990   \n5          0.520891         0.637915             0.579403      0.328647   \n6          0.517413         0.620033             0.568723      0.184725   \n7          0.486842         0.636537             0.561689      0.326279   \n8          0.536458         0.620290             0.578374      0.343907   \n9          0.430986         0.623413             0.527200      0.534031   \n10         0.282353         0.810056             0.546204      0.320000   \n11         0.255973         0.786738             0.521356      0.239617   \n12         0.222222         0.806854             0.514538      0.358621   \n13         0.305970         0.798444             0.552207      0.136667   \n14         0.256798         0.810662             0.533730      0.292096   \n15         0.130000         0.892092             0.511046      0.165605   \n16         0.181034         0.885538             0.533286      0.124260   \n17         0.181102         0.890337             0.535720      0.138554   \n18         0.243243         0.892185             0.567714      0.056962   \n19         0.137931         0.893976             0.515953      0.153846   \n\n    val_recall_1  val_recall_macro  \n0       0.760479          0.626553  \n1       0.744578          0.635872  \n2       0.735099          0.636284  \n3       0.777549          0.667091  \n4       0.738860          0.640539  \n5       0.637915          0.562548  \n6       0.620033          0.535034  \n7       0.636537          0.550292  \n8       0.620290          0.563417  \n9       0.623413          0.528245  \n10      0.810056          0.550485  \n11      0.786738          0.520356  \n12      0.806854          0.519801  \n13      0.798444          0.526778  \n14      0.810662          0.537006  \n15      0.892092          0.513590  \n16      0.885538          0.523824  \n17      0.890337          0.528170  \n18      0.892185          0.517379  \n19      0.893976          0.517541  \n\n[20 rows x 27 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.flush_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{'learning_rates': [0.02], 'batch_sizes': [7, 9], 'kernels_counts': [8, 9], 'sentences_counts': [2, 8], 'hidden_layers_1': [2, 5], 'alphas': [0.02, 0.027], 'regularization_types': ['L1']}\n<zip object at 0x7eff4825ee60>\n<zip object at 0x7eff482c9d20>\n<zip object at 0x7eff482c9aa0>\n<zip object at 0x7eff482c9640>\n<zip object at 0x7eff482c9190>\n<zip object at 0x7eff482c9f00>\n<zip object at 0x7eff482c9690>\n"
    }
   ],
   "source": [
    "models_data = json.load(open('/home/mibo/Projects/PERO/persil/modelsData', 'r'))\n",
    "models_parameters = models_data['models_parameters']\n",
    "print(models_parameters)\n",
    "a = [(label * len(values), values) for label, values in models_parameters.items()] \n",
    "for k in a:\n",
    "    print(k)"
   ]
  }
 ]
}