{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "import torch as to\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as func\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader\n",
    "import random \n",
    "import time \n",
    "import torch.nn.utils.rnn as rnnutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchviz\n",
    "from torchsummary import summary\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score \n",
    "from itertools import tee\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Info \n",
    "This notebook is a continuation of the 'baseline.ipynb' nootebook. \n",
    "\n",
    "### Setup \n",
    "+> Added balancing of the data-set. Balancing is done in a way that samples are added until the less frequent class isn't same as more frequent class. Samples are added squentialy, not randomly, thus ensuring that the differenece of the most frequent sample and least frequent sample in the data-set is one.\n",
    "\n",
    "+> f1 is now macro\n",
    "\n",
    "+> batchnormalization was added befor convolution and removed :D \n",
    "\n",
    "+> we are currently stopping the training if f1 is not changing (max_constant_f1 denotes the max number of epochs that the f1 is not changing)\n",
    "\n",
    "+> additonal data was added to the results dataframe (precision and recall for each class)\n",
    "\n",
    "+> to ease the strain on memory this code has all input data is represented as indices, thus it runs a bit slower because it has to access the input dataframe to attain input vectors\n",
    "\n",
    "+> original dataframe is preprocesed and deleted so that memory is freed \n",
    "\n",
    "+> now you can specify a gpu, ouput of all results, predictions and models (CONSTANT: 'MODELS_NAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0d7798c830>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CELL METADATA\n",
    "DATASET = '/home/mbosnjak/Datasets/embedded_comments_last_100_min_20com_per_auth_w_wc_10_200_no_mbti.csv'\n",
    "FOLDS = '/home/mbosnjak/Datasets/comments_last_100_min_20com_per_auth_w_wc_10_200.csv.folds.csv'\n",
    "AUTHORS = '/home/mbosnjak/Datasets/author_profiles.csv'\n",
    "\n",
    "MODELS_NAME = 'baseline_take2_twin'\n",
    "OUTPUT_DIR = f'mpr_{MODELS_NAME}/'\n",
    "RESULTS = OUTPUT_DIR + f'results_{MODELS_NAME}.csv'\n",
    "\n",
    "RELEVANT_DIFFERENCE = 0.00001\n",
    "NUMBER_OF_CLASSES = 2\n",
    "STATE = 156\n",
    "no_of_folds = 2\n",
    "cuda_device_index = 1\n",
    "\n",
    "mbti_labels_1 = ['introverted', 'intuitive']\n",
    "mbti_labels_2 = ['thinking', 'perceiving']\n",
    "mbti_labels = mbti_labels_2 #+ mbti_labels_1\n",
    "MODELS_PREFORMANCE_COLUMNS = ['val_f1', 'val_precision_0', 'val_precision_1', 'val_precision_macro' ,\\\n",
    "                              'val_recall_0', 'val_recall_1', 'val_recall_macro', \\\n",
    "                              'test_f1', 'test_precision_0', 'test_precision_1', 'test_precision_macro',\\\n",
    "                              'test_recall_0', 'test_recall_1', 'test_recall_macro',\\\n",
    "                              'epoch']\n",
    "MODELS_PARAMETER_COLUMNS = ['mbti_trait', 'fold', 'learning_rate', 'batch_size',\\\n",
    "                           'kernels_count', 'sentences_count' ]\n",
    "MODELS_META_DATA_COLUMNS = ['models_name', 'hash_id']\n",
    "to.manual_seed(STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CELL PARAMETERS\n",
    "batch_sizes = [1, 4, 16, 32]\n",
    "learning_rates = [0.001, 0.01, 0.0001, 0.00001, 0.000001, 0.0000001, 0.00000001, 0.000000001, 0.0000000001]\n",
    "kernel_sizes = [4, 8, 12, 16]\n",
    "sentences_counts = [2, 4]\n",
    "n_epochs = 30\n",
    "use_GPU = True\n",
    "#TODO: this is sentences per author not comments per author, quick fix is max size 200*100 (200 max sentences by comment * 100 comments)\n",
    "comments_per_author = 20000\n",
    "#comments_per_author = 20\n",
    "cuda_device = to.device(cuda_device_index) if use_GPU else None\n",
    "validation_p = .2\n",
    "print_status_batch = 1000\n",
    "max_constant_f1 = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_checkpoints_name(models_identifier):\n",
    "    return OUTPUT_DIR+\"save_\"+MODELS_NAME+'_'+models_identifier + \".pt\"\n",
    "\n",
    "def get_predictions_file_name(models_identifier, target, fold):\n",
    "    return OUTPUT_DIR + f'{target}_{fold}_{MODELS_NAME}_{models_identifier}_predictions'\n",
    "\n",
    "def delimiter():\n",
    "    print('-'*23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(DATASET)\n",
    "#input_df = pd.read_csv(DATASET, nrows = 2000) # TODO: remove nrows\n",
    "folds_df = pd.read_csv(FOLDS, usecols =['author', 'fold'])\n",
    "authors_profiles_df = pd.read_csv(AUTHORS)\n",
    "authors_with_mbti = list(authors_profiles_df[authors_profiles_df.introverted.notnull()]['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: add comment delimiter -> to recognize comments end and beginning\n",
    "def merger(comments):\n",
    "    npad = [(0, max(sentences_counts)), (0, 0)]\n",
    "    carpet = comments[:comments_per_author].as_matrix(columns=comments.columns[1:1025])\n",
    "    carpet_padded = np.pad(carpet, pad_width=npad, mode='constant', constant_values=0)\n",
    "    #print(f'before {carpet.shape} - after {carpet_padded.shape}')\n",
    "    return to.tensor(carpet_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbosnjak/.conda/envs/pero/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Data preparation\n",
    "data_df = input_df[input_df['author'].isin(authors_with_mbti)]\n",
    "del input_df\n",
    "present_authors = sorted(data_df['author'].unique())\n",
    "authors_indices = dict(zip(present_authors , list(range(len(present_authors)))))\n",
    "input_x_df = data_df.sort_values(by=['author', 'Unnamed: 0']).groupby(['author']).apply(merger).tolist()\n",
    "del data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(data=None, columns=MODELS_META_DATA_COLUMNS+MODELS_PREFORMANCE_COLUMNS+MODELS_PARAMETER_COLUMNS)\n",
    "for name in MODELS_PREFORMANCE_COLUMNS:\n",
    "    results = results.astype({name: 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hash(index):\n",
    "    global results\n",
    "    identifier = hashlib.md5(''.join([str(x) for x in results[MODELS_PARAMETER_COLUMNS].iloc[index]]).encode('utf-8')).hexdigest()\n",
    "    results.at[index, 'hash_id'] = identifier\n",
    "    return identifier\n",
    "\n",
    "def create_new_model_input(mbti_trait, fold, learning_rate, batch_size,\\\n",
    "                           kernels_count, sentences_count):\n",
    "    global results\n",
    "    results = results.append({'models_name':MODELS_NAME, 'mbti_trait' : mbti_trait, 'fold' : fold, 'learning_rate' : learning_rate, 'batch_size' : batch_size, 'kernels_count' : kernels_count, 'sentences_count' : sentences_count}, ignore_index=True)\n",
    "    return set_hash(len(results)-1)    \n",
    "\n",
    "def update_models_val_results(models_identifier, val_f1, val_precision_0, val_precision_1, val_precision_m, val_recall_0, val_recall_1, val_recall_m, epoch):\n",
    "    global results\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_f1'] = val_f1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_precision_0'] = val_precision_0\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_precision_1'] = val_precision_1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_precision_macro'] = val_precision_m\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_recall_0'] = val_recall_0\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_recall_1'] = val_recall_1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'val_recall_macro'] = val_recall_m\n",
    "    results.loc[results['hash_id'] == models_identifier, 'epoch'] = epoch\n",
    "    \n",
    "def update_models_test_results(models_identifier, test_f1, test_precision_0, test_precision_1, precision_m, test_recall_0, test_recall_1, test_recall_m):\n",
    "    global results\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_f1'] = test_f1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_precision_0'] = test_precision_0\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_precision_1'] = test_precision_1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_precision_macro'] = test_precision_m\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_recall_0'] = test_recall_0\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_recall_1'] = test_recall_1\n",
    "    results.loc[results['hash_id'] == models_identifier, 'test_recall_macro'] = test_recall_m\n",
    "    \n",
    "def get_best_models_data(target, fold):\n",
    "    return results.loc[results[(results.mbti_trait == target) & (results.fold == fold)]['val_f1'].idxmax()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e5a677ccc00e12a5dc84842655a04262'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_new_model_input('dfsfdsf',333,3,223333,1,13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output(data_authors, target):\n",
    "    output_prep = authors_profiles_df[authors_profiles_df['author'].isin(data_authors)][target].tolist()\n",
    "    input_indices = [index for author, index in authors_indices.items() if author in data_authors]\n",
    "    return input_indices, output_prep\n",
    "\n",
    "def balance_binary_data(input_df, output_df):\n",
    "    positive_indices = [index for index, element in enumerate(output_df) if element == 1.0]\n",
    "    negative_indices = [index for index, element in enumerate(output_df) if element == 0.0]\n",
    "    negative_count = len(negative_indices)\n",
    "    positive_count = len(positive_indices)\n",
    "    \n",
    "    if(positive_count > negative_count):\n",
    "        return create_balanced_data(positive_indices, negative_indices, input_df, output_df)\n",
    "    elif(negative_count > positive_count):\n",
    "        return create_balanced_data(negative_indices, positive_indices, input_df, output_df)\n",
    "        \n",
    "def create_balanced_data(more_frequent, less_frequent, input_df, output_df):\n",
    "    more_frequent_count = len(more_frequent)\n",
    "    less_frequent_count = len(less_frequent)\n",
    "    constant_multiplyer = more_frequent_count // less_frequent_count\n",
    "    remaining_additions = more_frequent_count % less_frequent_count\n",
    "    balanced_indices = less_frequent*constant_multiplyer + less_frequent[:remaining_additions] + more_frequent\n",
    "    output_df_balanced = [output_df[index] for index in balanced_indices]\n",
    "    input_df_balanced = [input_df[index] for index in balanced_indices]\n",
    "    return input_df_balanced, output_df_balanced\n",
    "    \n",
    "def data_preparation(target, fold):\n",
    "    test_data_authors = folds_df[(folds_df['fold'] == fold) & (folds_df['author'].isin(present_authors))]['author'].tolist()\n",
    "    train_data_authors = folds_df[(folds_df['fold'] != fold) & (folds_df['author'].isin(present_authors))]['author'].tolist()\n",
    "    train_input_indices, train_output = get_input_output(train_data_authors, target)\n",
    "    test_input_indices, test_output = get_input_output(test_data_authors, target)\n",
    "\n",
    "    train_input_indices, val_input_indices, train_output, val_output = train_test_split(train_input_indices, train_output, test_size=validation_p, random_state=STATE)\n",
    "    assert len(train_input_indices) == len(train_output)\n",
    "    assert len(test_input_indices) == len(test_output)\n",
    "    assert len(val_input_indices) == len(val_output)\n",
    "    train_input_indices, train_output = balance_binary_data(train_input_indices, train_output)\n",
    "    return train_input_indices, train_output, val_input_indices, val_output, test_input_indices, test_output\n",
    "\n",
    "def chunks(l, n):\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]  \n",
    "\n",
    "def create_minibatches(data_X, data_y, minibatch_size, cuda_dev):\n",
    "    for idx_list in chunks(range(len(data_X)), minibatch_size):\n",
    "        data_X_indices = [data_X[index] for index in idx_list]\n",
    "        data_y_idx = [int(data_y[index]) for index in idx_list]\n",
    "        \n",
    "        minibatch_X = [input_x_df[index] for index in data_X_indices]\n",
    "        minibatch_X = rnnutils.pad_sequence(minibatch_X, batch_first=True, padding_value = 0) \n",
    "        minibatch_X = minibatch_X.unsqueeze(1)\n",
    "        minibatch_y = to.tensor(data_y_idx)\n",
    "        if cuda_dev is not None:\n",
    "            minibatch_X = minibatch_X#.to(device=cuda_dev, dtype=to.float)\n",
    "            minibatch_y = minibatch_y.to(device=cuda_dev, dtype=to.long)\n",
    "\n",
    "        yield((minibatch_X, minibatch_y))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvoCarpet(nn.Module):\n",
    "    def __init__ (self, embedding_size = 1024, kernels_count=64, sentences_count=2):\n",
    "        super(ConvoCarpet, self).__init__()\n",
    "        self.batch_norm = nn.BatchNorm2d(1)\n",
    "        self.conv_layer = nn.Conv2d(1, kernels_count, [sentences_count, embedding_size])\n",
    "        self.pool_layer = nn.AdaptiveMaxPool2d((1, None))\n",
    "        self.fc_layer = nn.Linear(kernels_count, NUMBER_OF_CLASSES)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_batch):\n",
    "        #normalized_input = self.batch_norm(input_batch)\n",
    "        conv_output = func.relu(self.conv_layer(input_batch))\n",
    "        maxpool_output = self.pool_layer(conv_output)\n",
    "        maxpool_output = maxpool_output.flatten(start_dim=1)\n",
    "        linear_output = self.fc_layer(maxpool_output)\n",
    "        output = self.softmax(linear_output)\n",
    "        #return func.log_softmax(linear_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcMeasures(total_logits, total_true, threshold):\n",
    "    total_preds = []\n",
    "    for minibatch_logits in total_logits:\n",
    "        minibatch_preds = minibatch_logits[:,1] > threshold\n",
    "        total_preds.append(minibatch_preds)\n",
    "    total_preds = to.cat(total_preds)\n",
    "    total_true = to.cat(total_true)\n",
    "    F1 = f1_score(total_true.cpu(), total_preds.cpu(), pos_label = 1, average='macro')\n",
    "    precision_m = precision_score(total_true.cpu(), total_preds.cpu(), average='macro')\n",
    "    precision_1 = precision_score(total_true.cpu(), total_preds.cpu(), pos_label = 1)\n",
    "    precision_0 = precision_score(total_true.cpu(), total_preds.cpu(), pos_label = 0)\n",
    "    recall_m = recall_score(total_true.cpu(), total_preds.cpu(), average='macro')\n",
    "    recall_1 = recall_score(total_true.cpu(), total_preds.cpu(), pos_label = 1)\n",
    "    recall_0 = recall_score(total_true.cpu(), total_preds.cpu(), pos_label = 0)\n",
    "    return F1, precision_0, precision_1, precision_m, recall_0, recall_1, recall_m\n",
    "\n",
    "def apply_model(model, loss_func, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predicted = []\n",
    "    true_output = []\n",
    "    minibatch_loss = 0\n",
    "    with to.no_grad():\n",
    "        n_batch = 0\n",
    "        for inputs, labels in data_loader:\n",
    "            n_batch += 1\n",
    "            minibatch_logits = model(inputs.to(device=cuda_device, dtype=to.float))\n",
    "            minibatch_loss = loss_func(minibatch_logits, labels)\n",
    "            \n",
    "            total_loss += minibatch_loss\n",
    "            predicted.append(minibatch_logits)\n",
    "            true_output.append(labels)\n",
    "            \n",
    "    return total_loss, predicted, true_output\n",
    "\n",
    "def optimize_models_params(trait, fold, train_input_indices, train_output, val_input_indices, val_output):\n",
    "    print('Start training ...')\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        for kernel_size in kernel_sizes:\n",
    "            for sent_size in sentences_counts:\n",
    "                for learning_rate in learning_rates:                \n",
    "                    \n",
    "                    train_loader = create_minibatches(train_input_indices, train_output, batch_size, cuda_device)\n",
    "                    val_loader = create_minibatches(val_input_indices, val_output, batch_size, cuda_device)\n",
    "                    \n",
    "                    delimiter()\n",
    "                    delimiter()\n",
    "                    print(f'Starting training for BS: {batch_size} KS: {kernel_size} SS: {sent_size} LR: {learning_rate}')\n",
    "                    delimiter()\n",
    "                    \n",
    "                    models_identifier = create_new_model_input(trait, fold, learning_rate, batch_size, kernel_size, sent_size)\n",
    "                    \n",
    "                    model = ConvoCarpet(kernels_count=kernel_size, sentences_count=sent_size)\n",
    "                    if use_GPU:\n",
    "                          model.to(cuda_device)\n",
    "\n",
    "                    optimizer = to.optim.Adam(model.parameters(), learning_rate, amsgrad = True)\n",
    "\n",
    "                    train_model(train_loader, val_loader, models_identifier, model, loss, optimizer)\n",
    "                    results.to_csv(RESULTS)\n",
    "    \n",
    "\n",
    "def train_model(train_loader, val_loader, models_identifier, model, loss, optimizer):\n",
    "    best_f1_val = 0\n",
    "    last_f1 = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(0, n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        n_batch = 0\n",
    "        train_loader, train_loader_backup = tee(train_loader)\n",
    "        for inputs, labels in train_loader_backup:\n",
    "\n",
    "            n_batch += 1\n",
    "            #print(f\"Current batch is {n_batch}\")\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            logits = model(inputs.to(device=cuda_device, dtype=to.float))\n",
    "            minibatch_loss = loss(logits, labels)\n",
    "            total_loss += minibatch_loss\n",
    "            minibatch_loss.backward()\n",
    "            optimizer.step()\n",
    "            if(n_batch % print_status_batch == 0):\n",
    "                \n",
    "                delimiter()\n",
    "                print(f\"Train - Epoch {epoch} - batch {n_batch}, batch loss is {minibatch_loss:.6f}, total loss is {total_loss:.6f}\")\n",
    "                delimiter()\n",
    "\n",
    "                \n",
    "        delimiter()\n",
    "        val_loader, val_loader_backup = tee(val_loader)\n",
    "        val_loss, val_logits, true_val = apply_model(model, loss, val_loader_backup)\n",
    "        val_f1_score, val_precision_0, val_precision_1, val_precision_m, val_recall_0, val_recall_1, val_recall_m = calcMeasures(val_logits, true_val, 0.5)\n",
    "        \n",
    "        \n",
    "        print(f'Epoch {epoch} end: {time.time()-epoch_start}')\n",
    "        print(f'Validation loss: {val_loss:.7f} - F1 score: {val_f1_score:.7f}')\n",
    "        print(f'0 class -> precision: {val_precision_0:.7f} - recall: {val_recall_0:.7f}')\n",
    "        print(f'1 class -> precision: {val_precision_1:.7f} - recall: {val_recall_1:.7f}')\n",
    "        print(f'precision: {val_precision_m:.7f} - recall: {val_recall_m:.7f} - MACRO')\n",
    "        delimiter()\n",
    "        \n",
    "        if (abs(val_f1_score - last_f1) <= RELEVANT_DIFFERENCE):\n",
    "            counter += 1\n",
    "        else:\n",
    "            counter = 0\n",
    "        \n",
    "        last_f1 = val_f1_score\n",
    "        \n",
    "        if(counter > max_constant_f1):\n",
    "            return\n",
    "        \n",
    "        if (val_f1_score > best_f1_val):\n",
    "            update_models_val_results(models_identifier, val_f1_score, val_precision_0, val_precision_1, val_precision_m, val_recall_0, val_precision_1, val_recall_m, epoch)\n",
    "            best_f1_val = val_f1_score\n",
    "        # save a checkpoint\n",
    "            best_checkpoint_filename = get_checkpoints_name(models_identifier)\n",
    "            to.save({\n",
    "              \"models_identifier\" : models_identifier, \n",
    "              \"model_state_dict\" : model.state_dict(),\n",
    "              \"optimizer_state_dict\" : optimizer.state_dict()\n",
    "            }, best_checkpoint_filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model for target: thinking\n",
      "Processing fold 0...\n",
      "Start training ...\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 0.001\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.313273, total loss is 314.536163\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.313278, total loss is 627.818726\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.313288, total loss is 941.108765\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 1.313241, total loss is 1631.354614\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.313262, total loss is 2504.381836\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.313262, total loss is 2817.614258\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.313262, total loss is 3130.846680\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 21.2665593624115\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 1 end: 14.67478346824646\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.343268394470215\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 15.86996579170227\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 4 end: 17.052088260650635\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 0.01\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.313313, total loss is 349.772491\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.313262, total loss is 663.038818\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.313262, total loss is 976.285583\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 1.313261, total loss is 1666.519287\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.313262, total loss is 2748.939941\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.313262, total loss is 3062.172363\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.313262, total loss is 3375.404785\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 18.713940858840942\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Epoch 1 end: 15.349604606628418\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.4243483543396\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 15.514164686203003\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 1.313262, total loss is 1313.244263\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 1.313262, total loss is 2626.476807\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 1.313262, total loss is 3939.709229\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 0.313262, total loss is 4876.156738\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.313262, total loss is 5189.633301\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.313262, total loss is 5503.109863\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.313262, total loss is 5816.586426\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 4 end: 15.44417929649353\n",
      "Validation loss: 890.9356689 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 0.0001\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.313526, total loss is 323.620544\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.313408, total loss is 637.263428\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.313424, total loss is 950.703247\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 1.312316, total loss is 1640.866577\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.314834, total loss is 2623.222168\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.314015, total loss is 2938.399658\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.314045, total loss is 3252.420898\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 23.12838578224182\n",
      "Validation loss: 890.7942505 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 1.311815, total loss is 1311.246826\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 0.313919, total loss is 1927.014526\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 0.313784, total loss is 2241.061523\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 1.312229, total loss is 2931.012695\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.319923, total loss is 3758.280029\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.315037, total loss is 4075.399902\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.314361, total loss is 4390.098145\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 1 end: 15.61073350906372\n",
      "Validation loss: 890.7566528 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 1.259208, total loss is 1303.334473\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 0.315716, total loss is 1754.177246\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 0.313344, total loss is 2068.588135\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 1.311081, total loss is 2758.307373\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.319427, total loss is 3552.292236\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.316822, total loss is 3870.572510\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.317109, total loss is 4185.751465\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.057515382766724\n",
      "Validation loss: 890.6535034 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 1.248393, total loss is 1297.699219\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 0.316644, total loss is 1708.915649\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 0.313718, total loss is 2023.103149\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 1.311708, total loss is 2712.901855\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.324668, total loss is 3534.382568\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.316361, total loss is 3854.023438\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.314167, total loss is 4169.565918\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 16.908008813858032\n",
      "Validation loss: 890.6162109 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 1.189385, total loss is 1293.090698\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 0.314422, total loss is 1699.052612\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 0.313344, total loss is 2012.830322\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 1.311353, total loss is 2702.719238\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.322381, total loss is 3442.766357\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.319256, total loss is 3762.499023\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.314864, total loss is 4078.221436\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 4 end: 17.507537364959717\n",
      "Validation loss: 890.4360962 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 1e-05\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.365211, total loss is 494.484711\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.339095, total loss is 849.170837\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.330632, total loss is 1180.670532\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 1.265232, total loss is 1864.469971\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.689428, total loss is 2979.040039\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.421161, total loss is 3474.129883\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.367990, total loss is 3850.995605\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 20.11070203781128\n",
      "Validation loss: 885.6419067 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 0.823414, total loss is 1106.213745\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 0.375972, total loss is 1652.697998\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 0.336068, total loss is 2002.769043\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 1.198228, total loss is 2680.403809\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.507377, total loss is 3597.863770\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.416618, total loss is 4042.907959\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.381662, total loss is 4423.727539\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 1 end: 16.846572637557983\n",
      "Validation loss: 882.7653809 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 0.869718, total loss is 1104.333740\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 0.380989, total loss is 1683.761963\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 0.336281, total loss is 2036.468018\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 1.191471, total loss is 2712.474609\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.475576, total loss is 3592.507812\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.411024, total loss is 4024.365479\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.381251, total loss is 4402.662598\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.481269598007202\n",
      "Validation loss: 881.5911865 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 0.846497, total loss is 1105.650024\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 0.384976, total loss is 1659.687378\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 0.336832, total loss is 2010.576538\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 1.203955, total loss is 2685.148926\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.459230, total loss is 3550.856201\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.401688, total loss is 3972.087646\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.381099, total loss is 4345.843262\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 15.050564765930176\n",
      "Validation loss: 880.8086548 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 0.730499, total loss is 1096.429810\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 0.377757, total loss is 1596.816528\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 0.332755, total loss is 1943.187500\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 1.197809, total loss is 2616.881592\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.445968, total loss is 3475.942139\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.387984, total loss is 3885.360596\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.375239, total loss is 4251.522461\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 4 end: 15.06895112991333\n",
      "Validation loss: 880.5419312 - F1 score: 0.4058824\n",
      "0 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "1 class -> precision: 0.6831683 - recall: 1.0000000\n",
      "precision: 0.3415842 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 1e-06\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.653014, total loss is 677.072266\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.604370, total loss is 1306.350342\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.543992, total loss is 1891.995117\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 0.866474, total loss is 2560.059082\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.781751, total loss is 3375.162354\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.703855, total loss is 4134.833496\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.716696, total loss is 4842.217285\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 20.621358394622803\n",
      "Validation loss: 969.2320557 - F1 score: 0.4657647\n",
      "0 class -> precision: 0.3030303 - recall: 0.1116071\n",
      "1 class -> precision: 0.6813451 - recall: 0.8809524\n",
      "precision: 0.4921877 - recall: 0.4962798 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 0.676416, total loss is 692.784180\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 0.627053, total loss is 1338.777954\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 0.561171, total loss is 1943.097778\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 0.841040, total loss is 2612.950928\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.756644, total loss is 3403.843994\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.686555, total loss is 4142.741699\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.699776, total loss is 4831.428711\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 1 end: 15.152265787124634\n",
      "Validation loss: 960.4350586 - F1 score: 0.4263641\n",
      "0 class -> precision: 0.3243243 - recall: 0.0267857\n",
      "1 class -> precision: 0.6833696 - recall: 0.9741201\n",
      "precision: 0.5038470 - recall: 0.5004529 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 0.697833, total loss is 708.427368\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 0.644291, total loss is 1368.636841\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 0.571327, total loss is 1985.417114\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 0.825905, total loss is 2656.202637\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.737779, total loss is 3431.828369\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.675398, total loss is 4156.273926\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.687732, total loss is 4831.575195\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.317869424819946\n",
      "Validation loss: 954.1016235 - F1 score: 0.4064932\n",
      "0 class -> precision: 0.1250000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6820768 - recall: 0.9927536\n",
      "precision: 0.4035384 - recall: 0.4974929 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 0.714432, total loss is 719.626953\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 0.657961, total loss is 1389.936768\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 0.580110, total loss is 2015.536133\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 0.810745, total loss is 2686.820068\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.723383, total loss is 3451.259521\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.667641, total loss is 4165.042969\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.678987, total loss is 4830.399414\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 14.928382396697998\n",
      "Validation loss: 949.4236450 - F1 score: 0.4072601\n",
      "0 class -> precision: 0.2000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6827537 - recall: 0.9958592\n",
      "precision: 0.4413769 - recall: 0.4990457 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 0.726170, total loss is 727.787109\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 0.671788, total loss is 1405.284790\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 0.586021, total loss is 2037.048950\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 0.798419, total loss is 2708.444580\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.712614, total loss is 3464.905273\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.662126, total loss is 4170.916016\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.669972, total loss is 4828.913574\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 4 end: 14.911552906036377\n",
      "Validation loss: 945.8813477 - F1 score: 0.4080254\n",
      "0 class -> precision: 0.5000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6834278 - recall: 0.9989648\n",
      "precision: 0.5917139 - recall: 0.5005985 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 1000, batch loss is 0.735192, total loss is 733.575378\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 2000, batch loss is 0.683956, total loss is 1415.940308\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 3000, batch loss is 0.590419, total loss is 2051.808105\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 4000, batch loss is 0.788894, total loss is 2723.093994\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 5000, batch loss is 0.704097, total loss is 3473.721924\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 6000, batch loss is 0.658284, total loss is 4173.810059\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 5 - batch 7000, batch loss is 0.663308, total loss is 4826.212891\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 5 end: 14.843722343444824\n",
      "Validation loss: 943.0966187 - F1 score: 0.4080254\n",
      "0 class -> precision: 0.5000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6834278 - recall: 0.9989648\n",
      "precision: 0.5917139 - recall: 0.5005985 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 1000, batch loss is 0.742036, total loss is 737.602905\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 2000, batch loss is 0.693839, total loss is 1423.343994\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 3000, batch loss is 0.593134, total loss is 2061.947021\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 4000, batch loss is 0.780783, total loss is 2732.991455\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 5000, batch loss is 0.696862, total loss is 3479.140381\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 6000, batch loss is 0.655425, total loss is 4174.479980\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 6 - batch 7000, batch loss is 0.657653, total loss is 4822.314453\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 6 end: 14.978867769241333\n",
      "Validation loss: 940.7161255 - F1 score: 0.4080254\n",
      "0 class -> precision: 0.5000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6834278 - recall: 0.9989648\n",
      "precision: 0.5917139 - recall: 0.5005985 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 1000, batch loss is 0.747443, total loss is 740.726074\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 2000, batch loss is 0.701737, total loss is 1429.003052\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 3000, batch loss is 0.595483, total loss is 2069.520752\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 4000, batch loss is 0.773658, total loss is 2740.217529\n",
      "-----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------\n",
      "Train - Epoch 7 - batch 5000, batch loss is 0.692232, total loss is 3482.989258\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 6000, batch loss is 0.653302, total loss is 4174.682129\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 7 - batch 7000, batch loss is 0.653024, total loss is 4818.866211\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 7 end: 14.972870349884033\n",
      "Validation loss: 938.7424927 - F1 score: 0.4080254\n",
      "0 class -> precision: 0.5000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6834278 - recall: 0.9989648\n",
      "precision: 0.5917139 - recall: 0.5005985 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 1000, batch loss is 0.751035, total loss is 742.987915\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 2000, batch loss is 0.707142, total loss is 1432.931396\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 3000, batch loss is 0.595849, total loss is 2074.518799\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 4000, batch loss is 0.768213, total loss is 2744.737549\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 5000, batch loss is 0.688615, total loss is 3485.109619\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 6000, batch loss is 0.651750, total loss is 4173.976074\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 8 - batch 7000, batch loss is 0.649122, total loss is 4815.207031\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 8 end: 15.364770889282227\n",
      "Validation loss: 937.1098022 - F1 score: 0.4080254\n",
      "0 class -> precision: 0.5000000 - recall: 0.0022321\n",
      "1 class -> precision: 0.6834278 - recall: 0.9989648\n",
      "precision: 0.5917139 - recall: 0.5005985 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "-----------------------\n",
      "Starting training for BS: 1 KS: 4 SS: 2 LR: 1e-07\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 1000, batch loss is 0.625017, total loss is 639.784729\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 2000, batch loss is 0.612355, total loss is 1276.752686\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 3000, batch loss is 0.638190, total loss is 1910.855469\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 4000, batch loss is 0.768651, total loss is 2589.956055\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 5000, batch loss is 0.751377, total loss is 3346.093994\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 6000, batch loss is 0.751051, total loss is 4098.785645\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 0 - batch 7000, batch loss is 0.760632, total loss is 4847.818359\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 0 end: 19.940418243408203\n",
      "Validation loss: 1009.3875122 - F1 score: 0.2406015\n",
      "0 class -> precision: 0.3168317 - recall: 1.0000000\n",
      "1 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "precision: 0.1584158 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 1000, batch loss is 0.625083, total loss is 639.925903\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 2000, batch loss is 0.613332, total loss is 1277.321289\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 3000, batch loss is 0.638682, total loss is 1912.138672\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 4000, batch loss is 0.767449, total loss is 2591.364746\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 5000, batch loss is 0.750168, total loss is 3346.454346\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 6000, batch loss is 0.749951, total loss is 4098.204102\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 1 - batch 7000, batch loss is 0.759536, total loss is 4846.322266\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 1 end: 15.555639743804932\n",
      "Validation loss: 1008.8636475 - F1 score: 0.2406015\n",
      "0 class -> precision: 0.3168317 - recall: 1.0000000\n",
      "1 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "precision: 0.1584158 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 1000, batch loss is 0.625621, total loss is 640.635620\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 2000, batch loss is 0.614411, total loss is 1278.737549\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 3000, batch loss is 0.639035, total loss is 1914.252075\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 4000, batch loss is 0.766446, total loss is 2593.575439\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 5000, batch loss is 0.749038, total loss is 3347.782471\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 6000, batch loss is 0.748893, total loss is 4098.654297\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 2 - batch 7000, batch loss is 0.758457, total loss is 4845.899414\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 2 end: 15.818294763565063\n",
      "Validation loss: 1008.3522949 - F1 score: 0.2406015\n",
      "0 class -> precision: 0.3168317 - recall: 1.0000000\n",
      "1 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "precision: 0.1584158 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 1000, batch loss is 0.626154, total loss is 641.327759\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 2000, batch loss is 0.615478, total loss is 1280.123047\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 3000, batch loss is 0.639374, total loss is 1916.314697\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 4000, batch loss is 0.765478, total loss is 2595.727295\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 5000, batch loss is 0.747924, total loss is 3349.075928\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 6000, batch loss is 0.747851, total loss is 4099.100098\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 3 - batch 7000, batch loss is 0.757411, total loss is 4845.488770\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 3 end: 15.79074764251709\n",
      "Validation loss: 1007.8561401 - F1 score: 0.2406015\n",
      "0 class -> precision: 0.3168317 - recall: 1.0000000\n",
      "1 class -> precision: 0.0000000 - recall: 0.0000000\n",
      "precision: 0.1584158 - recall: 0.5000000 - MACRO\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 1000, batch loss is 0.626713, total loss is 641.990173\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 2000, batch loss is 0.616543, total loss is 1281.446167\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 3000, batch loss is 0.639681, total loss is 1918.288086\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 4000, batch loss is 0.764528, total loss is 2597.787109\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 5000, batch loss is 0.746831, total loss is 3350.307129\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 6000, batch loss is 0.746811, total loss is 4099.501953\n",
      "-----------------------\n",
      "-----------------------\n",
      "Train - Epoch 4 - batch 7000, batch loss is 0.756394, total loss is 4845.050293\n",
      "-----------------------\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "for target in mbti_labels:\n",
    "    print(f\"Creating model for target: {target}\")\n",
    "    for fold in range(0, no_of_folds):\n",
    "        \n",
    "        print(f'Processing fold {fold}...')\n",
    "        \n",
    "        train_input_indices, train_output, val_input_indices, val_output, test_input_indices, test_output = data_preparation(target, fold)\n",
    "        \n",
    "        optimize_models_params(target, fold, train_input_indices, train_output, val_input_indices, val_output)\n",
    "        \n",
    "        \n",
    "        best_models = get_best_models_data(target, fold)\n",
    "        #Recreate model\n",
    "        models_identifier = best_models['hash_id']\n",
    "        model = ConvoCarpet(kernels_count=best_models['kernels_count'] , sentences_count=best_models['sentences_count'])\n",
    "        if use_GPU:\n",
    "              model.to(cuda_device)\n",
    "\n",
    "        checkpoint = to.load(get_checkpoints_name(models_identifier))\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        \n",
    "        print(\".\"*15)\n",
    "        print(\".\"*15)\n",
    "        \n",
    "        print(f'Apply best model to test for {target} on fold {fold}.')\n",
    "        test_loader = create_minibatches(test_input_indices, test_output, 1, cuda_device)\n",
    "        test_loss, test_logits, test_true = apply_model(model, loss, test_loader)\n",
    "        test_f1_score, test_precision_0, test_precision_1, test_precision_m, test_recall_0, test_recall_1, test_recall_m = calcMeasures(test_logits, test_true, 0.5)\n",
    "        print(f'Test loss: {test_loss:.5f} - F1 score: {test_f1_score:.7f} ')\n",
    "        print(f'0 class -> precision: {test_precision_0:.7f} - recall: {test_recall_0:.7f}')\n",
    "        print(f'1 class -> precision: {test_precision_1:.7f} - recall: {test_recall_1:.7f}')\n",
    "        print(f'precision: {test_precision_m:.7f} - recall: {test_recall_m:.7f} - MACRO')\n",
    "              \n",
    "        update_models_test_results(models_identifier, test_f1_score, test_precision_0, test_precision_1, test_precision_m, test_recall_0, test_recall_1, test_recall_m)\n",
    "        with open(get_predictions_file_name(models_identifier, target, fold), 'w+') as f:\n",
    "            for pre_l, true_l in zip(test_logits, test_true):\n",
    "                f.write(f'{pre_l.cpu()[0][0]}, {pre_l.cpu()[0][1]}, {true_l.cpu().tolist()[0]}\\n')\n",
    "        results.to_csv(RESULTS)\n",
    "        \n",
    "        ##Save best models data\n",
    "        \n",
    "        print(f\"+++ Finished with training and testing model for {target} on fold {fold}. +++\")\n",
    "        \n",
    "        print(\".\"*15)\n",
    "        print(\".\"*15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
